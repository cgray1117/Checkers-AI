{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cgray1117/Checkers-AI/blob/main/Lynn_Nyazika_CGAN_Source_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lynn Nyazika\n",
        "Code adapted from: \"Conditional GAN (cGAN) in PyTorch and TensorFlow\"\n",
        "\n",
        "Source: https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/\n"
      ],
      "metadata": {
        "id": "IEG23F56m-1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lxU_OcXsbMm"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RtV3QxBMMMpQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UqaXTkaZMcBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cd44b5-3590-487a-a134-c64a4a934d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get install git\n"
      ],
      "metadata": {
        "id": "BPR0D0MZRWLE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/Goitsemedi888/Conditional-Generative-Adversarial-Network.git\n",
        "#%cd Conditional-Generative-Adversarial-Network"
      ],
      "metadata": {
        "id": "d_8Mbdyqfonb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8_ZUzPN-GKz",
        "outputId": "cfcd3a56-24c0-46da-e8fc-fb38467399de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"My First Streamlit App\")\n",
        "st.write(\"Hello! This is working!\")\n",
        "st.button(\"Click Me!\")\n",
        "name = st.text_input(\"What's your name?\")\n",
        "if name:\n",
        "    st.write(f\"Hello {name}!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPRrUlBluQn1",
        "outputId": "bd70d5e6-39ba-4dd3-9fbc-bec96ccaef9e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVwxpEGz8lav",
        "outputId": "d1c531f7-9300-4249-ceed-e33731ea5d0c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdrive  sample_data  simple_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n",
        "!lt --port 8501 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_pdezkw8qy-",
        "outputId": "82f5867f-8139-410d-be72-c41a835780fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "changed 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0Kyour url is: https://tangy-actors-melt.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "try:\n",
        "    response = requests.get('https://loca.lt/mytunnelpassword')\n",
        "    print(\"Your tunnel password is:\", response.text.strip())\n",
        "except:\n",
        "    print(\"Couldn't get password automatically\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1PFR-oFAoLl",
        "outputId": "bf6401a2-4f43-4c0b-f3c6-389f8be52ca6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tunnel password is: 35.233.216.220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-yeTZmysnGE"
      },
      "source": [
        "# Install essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wna5k3sCNGPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "outputId": "18a1364e-1a99-4599-e143-0e0019ebebc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from mxnet) (2.32.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet) (2025.4.26)\n",
            "Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, graphviz, mxnet\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed graphviz-0.8.4 mxnet-1.9.1 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "895f5c1f14a84427b733a14cb3d4c837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.core==1.3.1 (from autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.core-1.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.features==1.3.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.3.1 (from autogluon.tabular[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.tabular-1.3.1-py3-none-any.whl.metadata (14 kB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#!pip install autogluon==0.0.15 autogluon.tabular \"mxnet<2.0.0\"\n",
        "!pip install mxnet\n",
        "!pip install autogluon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VORVFfPO-GhQ"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NVryGEjco6Y"
      },
      "outputs": [],
      "source": [
        "!pip install pyts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsIDsPlrMjsk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import autogluon as ag\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# MLP Classifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print(\"SKLEARN version:\", sklearn.__version__)\n",
        "#print(\"Seaborn version:\", sns.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVJlL50szIj"
      },
      "source": [
        "# Import Dataset RT_IOT_2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yMERsTofN3V"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/RT_IOT2022 DATA'\n",
        "rt_iot_data = pd.read_csv(f'{PATH}/RT_IOT2022.csv')\n",
        "rt_iot_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSpE9GxVQUJc"
      },
      "outputs": [],
      "source": [
        "# Check the existing columns in your DataFrame\n",
        "print(rt_iot_data.columns)\n",
        "\n",
        "# Drop the 'Unnamed: 0' column if it exists\n",
        "if 'Unnamed: 0' in rt_iot_data.columns:\n",
        "    rt_iot_data.drop(columns='Unnamed: 0', inplace=True)\n",
        "    print(\"Column 'Unnamed: 0' dropped successfully.\")\n",
        "else:\n",
        "    print(\"Column 'Unnamed: 0' not found in the DataFrame.\")\n",
        "\n",
        "print(rt_iot_data.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nJBcb7EF-UL"
      },
      "outputs": [],
      "source": [
        "# Replace '-' in the 'service' column with 'ntcptrd' not captured\n",
        "rt_iot_data['service'] = rt_iot_data['service'].replace('-', 'ntcptrd')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNTKbk06z7-w"
      },
      "outputs": [],
      "source": [
        "description = rt_iot_data.describe().T.style.background_gradient(\n",
        "    cmap='viridis',  # colormap\n",
        "    low=0, high=1,   # range of colors for the gradient\n",
        "    axis=None        # gradient across the entire DataFrame\n",
        ")\n",
        "\n",
        "# styling attributes\n",
        "styled_description = description.set_properties(**{'font-size': '10pt', 'text-align': 'center'}).bar(subset=['mean'], color='#FFA07A')\n",
        "\n",
        "styled_description"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.pairplot(rt_iot_data, vars=['flow_duration', 'fwd_pkts_tot', 'bwd_pkts_tot', 'fwd_data_pkts_tot', 'bwd_data_pkts_tot'], hue='Attack_type')\n",
        "plt.suptitle('Pairwise Relationships with Attack Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PqCsQR1wPIJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Attack_type', y='flow_duration', data=rt_iot_data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Flow Duration by Attack Type')\n",
        "plt.xlabel('Attack Type')\n",
        "plt.ylabel('Flow Duration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sm-ZmQiiPk08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring AutoML Library Autogluon QUICK rundown of dataset for direction on handling project."
      ],
      "metadata": {
        "id": "N_fZuhcpmq5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg0Eph12a_5K"
      },
      "outputs": [],
      "source": [
        "X = rt_iot_data.drop(columns=['Attack_type'])  # Features\n",
        "y = rt_iot_data['Attack_type']  # Labels\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-Hbi1dPuNk"
      },
      "outputs": [],
      "source": [
        "# Concatenate features and labels for training data\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Load data into Autogluon TabularDataset format\n",
        "train_data_autogluon = TabularDataset(train_data)\n",
        "\n",
        "# Train Autogluon model\n",
        "predictor = TabularPredictor(label='Attack_type').fit(train_data_autogluon)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_data_autogluon = TabularDataset(X_test)\n",
        "predictions = predictor.predict(test_data_autogluon)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = (predictions == y_test).mean()\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E0KZpETlaMc"
      },
      "outputs": [],
      "source": [
        "list(set(list(y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCyhzdn2p2MO"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data.bwd_header_size_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23f4r9g2HTGZ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data.flow_ECE_flag_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgtWp-mlIEOP"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data.bwd_URG_flag_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz8Q0tsFtBPQ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data.flow_CWR_flag_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngbxJR9uYCJz"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data[\"fwd_subflow_pkts\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7P4DCbHb8uw"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data[\"flow_iat.min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qd2HGY2cBUM"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data[\"fwd_pkts_payload.min\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2TOnMxQqYNS"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data[\"fwd_URG_flag_count\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-73W7d0czy3"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data.Attack_type, rt_iot_data[\"flow_RST_flag_count\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POgWGeey--xB"
      },
      "outputs": [],
      "source": [
        "# Group by 'Attack_type' and sum the specified columns\n",
        "attack_type_fwd_bwd_points = rt_iot_data.groupby('Attack_type')[['fwd_iat.min','flow_iat.min','fwd_pkts_payload.min','fwd_header_size_tot']].sum().reset_index()\n",
        "\n",
        "# Melt the DataFrame to make it suitable for Seaborn's lineplot\n",
        "attack_type_fwd_bwd_points_melted = attack_type_fwd_bwd_points.melt(id_vars='Attack_type', var_name='Feature', value_name='Sum')\n",
        "\n",
        "# Create the line plot using Seaborn\n",
        "plt.figure(figsize=(20, 8))\n",
        "sns.lineplot(data=attack_type_fwd_bwd_points_melted, x='Attack_type', y='Sum', hue='Feature')\n",
        "plt.xticks(rotation=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORZdVQdKlaOa"
      },
      "outputs": [],
      "source": [
        "attack_type_fwd_bwd_points = rt_iot_data.groupby('Attack_type')[['id.orig_p','fwd_pkts_per_sec','flow_pkts_payload.std','flow_iat.std', 'flow_pkts_payload.tot']].sum()\n",
        "#attack_type_fwd_bwd_points.plot(figsize=(15,5))\n",
        "figure = plt.subplots(figsize=(20,8))\n",
        "sns.lineplot(attack_type_fwd_bwd_points)\n",
        "plt.tight_layout()\n",
        "plt.xticks(rotation=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBbyPBTx0Eu1"
      },
      "outputs": [],
      "source": [
        "attack_type_fwd_flow_flag = rt_iot_data.groupby('Attack_type')[['fwd_PSH_flag_count','bwd_PSH_flag_count','fwd_URG_flag_count','bwd_URG_flag_count']].sum()\n",
        "figure = plt.subplots(figsize=(20,8))\n",
        "sns.lineplot(attack_type_fwd_flow_flag)\n",
        "plt.tight_layout()\n",
        "plt.xticks(rotation=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfcxMFeToIZU"
      },
      "outputs": [],
      "source": [
        "attack_type_fwd_bwd_points = rt_iot_data.groupby('Attack_type')[['id.orig_p','fwd_URG_flag_count',\"flow_iat.std\"]].sum()\n",
        "#attack_type_fwd_bwd_points.plot(figsize=(15,5))\n",
        "figure = plt.subplots(figsize=(20,8))\n",
        "sns.lineplot(attack_type_fwd_bwd_points)\n",
        "plt.tight_layout()\n",
        "plt.xticks(rotation=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nCi5AIaIolz"
      },
      "outputs": [],
      "source": [
        "rt_iot_data1 = rt_iot_data.drop(columns =(['bwd_URG_flag_count', 'flow_ECE_flag_count', 'id.resp_p', 'proto', 'fwd_bulk_rate','flow_CWR_flag_count','fwd_iat.tot',\n",
        "                                           'bwd_bulk_packets','fwd_bulk_bytes','fwd_bulk_packets','bwd_bulk_bytes','idle.avg','idle.max','bwd_pkts_payload.std',\n",
        "                                           'idle.std','bwd_pkts_tot','idle.min','fwd_pkts_payload.std','bwd_data_pkts_tot','idle.tot','bwd_pkts_payload.avg',\n",
        "                                           'bwd_pkts_payload.tot','bwd_iat.tot','active.std','bwd_subflow_bytes','bwd_init_window_size','fwd_iat.max',\n",
        "                                           'bwd_bulk_rate','fwd_iat.std','fwd_pkts_tot','bwd_subflow_pkts','bwd_iat.avg','bwd_iat.std','bwd_pkts_payload.min','id.orig_p']))\n",
        "rt_iot_data1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0ZxqwQddBEQ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data1.Attack_type, rt_iot_data1[\"flow_ACK_flag_count\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piJFIY1owO5_"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data1.Attack_type, rt_iot_data1[\"fwd_last_window_size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFU0Wl361hqZ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data1.Attack_type, rt_iot_data1[\"fwd_pkts_payload.avg\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j5USzBDvUTE"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(rt_iot_data1.Attack_type, rt_iot_data1[\"flow_iat.min\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Machine Learning Modules from SKLEARN to get a quick classification to use for SHAP feature selection."
      ],
      "metadata": {
        "id": "FwoAKYXcmT7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFlNMTBxWTov"
      },
      "outputs": [],
      "source": [
        "label = LabelEncoder()\n",
        "\n",
        "for col in rt_iot_data1.columns.values:\n",
        "    rt_iot_data1[col]= label.fit_transform(rt_iot_data1[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZduJwSxJtwl"
      },
      "outputs": [],
      "source": [
        "X = rt_iot_data1.drop(columns=['Attack_type'])  # Features\n",
        "y = rt_iot_data1['Attack_type']  # Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxLv_jkDCQc1"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtF4IMefWT_3"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Instantiate the XGBoost classifier with the specified parameters\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target labels for the test data\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate the classification report\n",
        "classification_report_op = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", classification_report_op)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ656HmOiQxy"
      },
      "outputs": [],
      "source": [
        "# Instantiate the MLP classifier with the specified parameters\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='tanh', solver='adam', random_state=42)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target labels for the test data\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate the classification report\n",
        "classification_report_op = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", classification_report_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning using ConditionalGAN(cGAN)"
      ],
      "metadata": {
        "id": "3NixnowEbvjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, Concatenate, Lambda, Flatten, Conv2D, Conv2DTranspose, Dropout, BatchNormalization\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n"
      ],
      "metadata": {
        "id": "lt7k3M3b1Is5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Simple TensorFlow code to check GPU availability and usage..."
      ],
      "metadata": {
        "id": "rZkRm4KR9Yx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EvaluationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, generator, discriminator, latent_dim, n_classes, validation_data, interval=100):\n",
        "        super(EvaluationCallback, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.validation_data = validation_data\n",
        "        self.interval = interval\n",
        "        self.generator_losses = []\n",
        "        self.discriminator_losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save losses\n",
        "        self.generator_losses.append(logs.get('generator_loss'))\n",
        "        self.discriminator_losses.append(logs.get('discriminator_loss'))\n",
        "\n",
        "        # Evaluate model every 'interval' epochs\n",
        "        if (epoch + 1) % self.interval == 0:\n",
        "            noise = np.random.normal(0, 1, (len(self.validation_data[0]), self.latent_dim))\n",
        "            sampled_labels = np.random.randint(0, self.n_classes, len(self.validation_data[0]))\n",
        "            generated_images = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "            # Get predictions from discriminator\n",
        "            predictions = self.discriminator.predict([generated_images, sampled_labels])\n",
        "            predicted_classes = (predictions > 0.5).astype(int)\n",
        "            true_classes = np.ones_like(predicted_classes)\n",
        "\n",
        "            # Calculate precision\n",
        "            precision = precision_score(true_classes, predicted_classes)\n",
        "            print(f'Precision at epoch {epoch + 1}: {precision}')\n",
        "\n",
        "            # Calculate and plot confusion matrix\n",
        "            cm = confusion_matrix(true_classes, predicted_classes)\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(cm, cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix at epoch {epoch + 1}')\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "            # Plot generator and discriminator losses\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(range(1, epoch + 2), self.generator_losses, label='Generator Loss')\n",
        "            plt.plot(range(1, epoch + 2), self.discriminator_losses, label='Discriminator Loss')\n",
        "            plt.title('Losses')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "def define_generator(latent_dim=128, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(64 * 64 * 3, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((64, 64, 3))(dense_output)\n",
        "\n",
        "    # Resize the image to the desired output size (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(reshaped_output)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers with dropout and batch normalization\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "def train_cgan(images, labels_encoded, epochs=250, batch_size=32):\n",
        "    latent_dim = 128\n",
        "    n_classes = len(np.unique(labels_encoded))\n",
        "    generator = define_generator(latent_dim, n_classes)\n",
        "    discriminator = define_discriminator()\n",
        "\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    generated_image = generator([noise_input, label_input])\n",
        "    validity = discriminator([generated_image, label_input])\n",
        "\n",
        "    cGAN = Model([noise_input, label_input], validity)\n",
        "    cGAN.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                 loss='binary_crossentropy')\n",
        "\n",
        "    checkpoint_path = \"rtiotcGAN_epoch_{epoch:04d}.h5\"\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        save_freq=1000,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    evaluation_callback = EvaluationCallback(generator, discriminator, latent_dim, n_classes,\n",
        "                                             validation_data=(images[-10:], labels_encoded[-10:]), interval=1000)\n",
        "\n",
        "    # Training loop\n",
        "    cGAN.fit([np.random.normal(0, 1, (len(images), latent_dim)), labels_encoded],\n",
        "             np.ones((len(images), 1)),\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size,\n",
        "             callbacks=[checkpoint_callback, evaluation_callback])\n",
        "\n",
        "    return generator, discriminator, cGAN\n",
        "\n",
        "# Usage Setup (5000 epochs is not a lot for GANs but runs for a considerable time)\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "generator, discriminator, cGAN = train_cgan(images, labels_encoded, epochs=250, batch_size=32)\n",
        "\n",
        "# Print Model Summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "cGAN.summary()\n"
      ],
      "metadata": {
        "id": "VtXpr66bhGUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvgFzuVWhGxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def define_generator(latent_dim=256, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(8 * 8 * 128, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((8, 8, 128))(dense_output)\n",
        "\n",
        "    # Upsample using transposed convolutional layers\n",
        "    upsample1 = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu')(reshaped_output)\n",
        "    upsample2 = Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='sigmoid')(upsample1)\n",
        "\n",
        "    # Resize the image to the desired output size (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(upsample2)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "def generate_images(model, test_input, n_classes=12):\n",
        "    # Initialize output variable to store generated images\n",
        "    output = None\n",
        "\n",
        "    # Generate images for each class label\n",
        "    for label in range(n_classes):\n",
        "        labels = np.ones((test_input.shape[0],)) * label\n",
        "        predictions = model.predict([test_input, labels])\n",
        "\n",
        "        if output is None:\n",
        "            output = predictions\n",
        "        else:\n",
        "            output = np.concatenate((output, predictions))\n",
        "\n",
        "    # Configure plotting parameters\n",
        "    nrow = n_classes\n",
        "    ncol = test_input.shape[0]\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    gs = fig.add_gridspec(nrow, ncol, wspace=0, hspace=0)\n",
        "\n",
        "    # Plot generated images\n",
        "    k = 0\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            ax = fig.add_subplot(gs[i, j])\n",
        "            ax.imshow(output[k])\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            ax.set_aspect('auto')\n",
        "            k += 1\n",
        "\n",
        "    # Show the generated images\n",
        "    plt.show()\n",
        "\n",
        "# Define and compile cGAN model\n",
        "latent_dim = 256\n",
        "n_classes = 12\n",
        "epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "generator = define_generator(latent_dim, n_classes)\n",
        "discriminator = define_discriminator()\n",
        "\n",
        "# Compile discriminator\n",
        "discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Combine generator and discriminator into cGAN\n",
        "noise_input = Input(shape=(latent_dim,))\n",
        "label_input = Input(shape=(1,))\n",
        "\n",
        "generated_image = generator([noise_input, label_input])\n",
        "validity = discriminator([generated_image, label_input])\n",
        "\n",
        "cGAN = Model([noise_input, label_input], validity)\n",
        "cGAN.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "             loss='binary_crossentropy')\n",
        "\n",
        "# Training loop\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "\n",
        "# Lists to store losses for plotting\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Train cGAN\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.randint(0, images.shape[0], batch_size)\n",
        "    real_images, labels = images[idx], labels_encoded[idx]\n",
        "\n",
        "    # Generate random noise for the generator input\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Generate fake images using the generator\n",
        "    generated_images = generator.predict([noise, labels])\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch([real_images, labels], np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch([generated_images, labels], np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator (via the cGAN model)\n",
        "    g_loss = cGAN.train_on_batch([noise, labels], np.ones((batch_size, 1)))\n",
        "\n",
        "    # Store losses for plotting\n",
        "    d_losses.append(d_loss[0])\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "    # Print training progress\n",
        "    print(f\"Epoch: {epoch + 1}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "    # Save generated images periodically\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        test_noise = np.random.normal(size=(10, latent_dim))\n",
        "        generate_images(generator, test_noise)\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), d_losses, label=\"Discriminator Loss\")\n",
        "plt.plot(range(1, epochs + 1), g_losses, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PMyW67qMkQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIABLE Model 1\n",
        "class EvaluationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, generator, discriminator, latent_dim, n_classes, validation_data, interval=100):\n",
        "        super(EvaluationCallback, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.validation_data = validation_data\n",
        "        self.interval = interval\n",
        "        self.generator_losses = []\n",
        "        self.discriminator_losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save losses\n",
        "        self.generator_losses.append(logs.get('generator_loss'))\n",
        "        self.discriminator_losses.append(logs.get('discriminator_loss'))\n",
        "\n",
        "        # Evaluate model every specified interval epochs\n",
        "        if (epoch + 1) % self.interval == 0:\n",
        "            # Generate images with the generator for validation\n",
        "            noise = np.random.normal(0, 1, (len(self.validation_data[0]), self.latent_dim))\n",
        "            sampled_labels = np.random.randint(0, self.n_classes, len(self.validation_data[0]))\n",
        "            generated_images = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "            # Get predictions from the discriminator\n",
        "            predictions = self.discriminator.predict([generated_images, sampled_labels])\n",
        "            predicted_classes = (predictions > 0.5).astype(int)\n",
        "            true_classes = np.ones_like(predicted_classes)\n",
        "\n",
        "            # Calculate precision\n",
        "            precision = precision_score(true_classes, predicted_classes)\n",
        "            print(f'Precision at epoch {epoch + 1}: {precision}')\n",
        "\n",
        "            # Calculate and plot confusion matrix\n",
        "            cm = confusion_matrix(true_classes, predicted_classes)\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(cm, cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix at epoch {epoch + 1}')\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "            # Plot generator and discriminator losses\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(range(1, epoch + 2), self.generator_losses, label='Generator Loss')\n",
        "            plt.plot(range(1, epoch + 2), self.discriminator_losses, label='Discriminator Loss')\n",
        "            plt.title('Losses')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def define_generator(latent_dim=128, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(64 * 64 * 3, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((64, 64, 3))(dense_output)\n",
        "\n",
        "    # Resize the image to the desired output size (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(reshaped_output)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "\n",
        "def train_cgan(images, labels_encoded, epochs=100, batch_size=32):\n",
        "    latent_dim = 128\n",
        "    n_classes = len(np.unique(labels_encoded))\n",
        "    generator = define_generator(latent_dim, n_classes)\n",
        "    discriminator = define_discriminator()\n",
        "\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    generated_image = generator([noise_input, label_input])\n",
        "    validity = discriminator([generated_image, label_input])\n",
        "\n",
        "    cGAN = Model([noise_input, label_input], validity)\n",
        "    cGAN.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                 loss='binary_crossentropy')\n",
        "\n",
        "    checkpoint_path = \"rtiotcGAN_epoch_{epoch:04d}.h5\"\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        save_freq='epoch',  # 'period' is deprecated\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    evaluation_callback = EvaluationCallback(generator, discriminator, latent_dim, n_classes,\n",
        "                                             validation_data=(images[-10:], labels_encoded[-10:]), interval=100)\n",
        "\n",
        "    # Training loop\n",
        "    cGAN.fit([np.random.normal(0, 1, (len(images), latent_dim)), labels_encoded],\n",
        "             np.ones((len(images), 1)),\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size,\n",
        "             callbacks=[checkpoint_callback, evaluation_callback])\n",
        "\n",
        "    return generator, discriminator, cGAN\n",
        "\n",
        "\n",
        "# Usage Setup (5000 epochs is a lot for GAN training and may require substantial compute resources)\n",
        "dataset_path = '/path/to/dataset'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "generator, discriminator, cGAN = train_cgan(images, labels_encoded, epochs=100, batch_size=32)\n",
        "\n",
        "# Print Model Summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "cGAN.summary()\n"
      ],
      "metadata": {
        "id": "OXWpR_OrShlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EvaluationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, generator, discriminator, latent_dim, n_classes, validation_data, interval=100):\n",
        "        super(EvaluationCallback, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.validation_data = validation_data\n",
        "        self.interval = interval\n",
        "        self.generator_losses = []\n",
        "        self.discriminator_losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save losses\n",
        "        self.generator_losses.append(logs.get('generator_loss'))\n",
        "        self.discriminator_losses.append(logs.get('discriminator_loss'))\n",
        "\n",
        "        # Evaluate model every 'interval' epochs\n",
        "        if (epoch + 100) % self.interval == 0:\n",
        "            noise = np.random.normal(0, 1, (len(self.validation_data[0]), self.latent_dim))\n",
        "            sampled_labels = np.random.randint(0, self.n_classes, len(self.validation_data[0]))\n",
        "            generated_images = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "            # Get predictions from discriminator\n",
        "            predictions = self.discriminator.predict([generated_images, sampled_labels])\n",
        "            predicted_classes = (predictions > 0.5).astype(int)\n",
        "            true_classes = np.ones_like(predicted_classes)\n",
        "\n",
        "            # Calculate precision\n",
        "            precision = precision_score(true_classes, predicted_classes)\n",
        "            print(f'Precision at epoch {epoch + 1}: {precision}')\n",
        "\n",
        "            # Calculate and plot confusion matrix\n",
        "            cm = confusion_matrix(true_classes, predicted_classes)\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(cm, cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix at epoch {epoch + 1}')\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "            # Plot generator and discriminator losses\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(range(1, epoch + 2), self.generator_losses, label='Generator Loss')\n",
        "            plt.plot(range(1, epoch + 2), self.discriminator_losses, label='Discriminator Loss')\n",
        "            plt.title('Losses')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def define_generator(latent_dim=128, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(64 * 64 * 3, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((64, 64, 3))(dense_output)\n",
        "\n",
        "    # Resize the image (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(reshaped_output)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "\n",
        "def train_cgan(images, labels_encoded, epochs=250, batch_size=32):\n",
        "    latent_dim = 128\n",
        "    n_classes = len(np.unique(labels_encoded))\n",
        "    generator = define_generator(latent_dim, n_classes)\n",
        "    discriminator = define_discriminator()\n",
        "\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    generated_image = generator([noise_input, label_input])\n",
        "    validity = discriminator([generated_image, label_input])\n",
        "\n",
        "    cGAN = Model([noise_input, label_input], validity)\n",
        "    cGAN.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                 loss='binary_crossentropy')\n",
        "\n",
        "    checkpoint_path = \"rtiotcGAN_epoch_{epoch:04d}.h5\"\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        save_freq=1000,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    evaluation_callback = EvaluationCallback(generator, discriminator, latent_dim, n_classes,\n",
        "                                             validation_data=(images[-10:], labels_encoded[-10:]), interval=1000)\n",
        "\n",
        "    # Training loop\n",
        "    cGAN.fit([np.random.normal(0, 1, (len(images), latent_dim)), labels_encoded],\n",
        "             np.ones((len(images), 1)),\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size,\n",
        "             callbacks=[checkpoint_callback, evaluation_callback])\n",
        "\n",
        "    return generator, discriminator, cGAN\n",
        "\n",
        "# Usage Setup (5000 epochs is not a lot for GANs but runs for a considerable time)\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "generator, discriminator, cGAN = train_cgan(images, labels_encoded, epochs=250, batch_size=32)\n",
        "\n",
        "# Print Model Summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "cGAN.summary()\n"
      ],
      "metadata": {
        "id": "KepJnvIoGs_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "\n",
        "class EvaluationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, generator, discriminator, latent_dim, n_classes, validation_data, interval=100):\n",
        "        super(EvaluationCallback, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.validation_data = validation_data\n",
        "        self.interval = interval\n",
        "        self.generator_losses = []\n",
        "        self.discriminator_losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save losses\n",
        "        self.generator_losses.append(logs.get('generator_loss'))\n",
        "        self.discriminator_losses.append(logs.get('discriminator_loss'))\n",
        "\n",
        "        # Evaluate model every specified interval epochs\n",
        "        if (epoch + 1) % self.interval == 0:\n",
        "            # Generate images with the generator for validation\n",
        "            noise = np.random.normal(0, 1, (len(self.validation_data[0]), self.latent_dim))\n",
        "            sampled_labels = np.random.randint(0, self.n_classes, len(self.validation_data[0]))\n",
        "            generated_images = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "            # Get predictions from the discriminator\n",
        "            predictions = self.discriminator.predict([generated_images, sampled_labels])\n",
        "            predicted_classes = (predictions > 0.5).astype(int)\n",
        "            true_classes = np.ones_like(predicted_classes)\n",
        "\n",
        "            # Calculate precision\n",
        "            precision = precision_score(true_classes, predicted_classes)\n",
        "            print(f'Precision at epoch {epoch + 1}: {precision}')\n",
        "\n",
        "            # Calculate and plot confusion matrix\n",
        "            cm = confusion_matrix(true_classes, predicted_classes)\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(cm, cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix at epoch {epoch + 1}')\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "            # Plot generator and discriminator losses\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(range(1, epoch + 2), self.generator_losses, label='Generator Loss')\n",
        "            plt.plot(range(1, epoch + 2), self.discriminator_losses, label='Discriminator Loss')\n",
        "            plt.title('Losses')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def define_generator(latent_dim=128, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(64 * 64 * 3, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((64, 64, 3))(dense_output)\n",
        "\n",
        "    # Resize the image to the desired output size (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(reshaped_output)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "\n",
        "def train_cgan(images, labels_encoded, epochs=100, batch_size=32):\n",
        "    latent_dim = 128\n",
        "    n_classes = len(np.unique(labels_encoded))\n",
        "    generator = define_generator(latent_dim, n_classes)\n",
        "    discriminator = define_discriminator()\n",
        "\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    generated_image = generator([noise_input, label_input])\n",
        "    validity = discriminator([generated_image, label_input])\n",
        "\n",
        "    cGAN = Model([noise_input, label_input], validity)\n",
        "    cGAN.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "                 loss='binary_crossentropy')\n",
        "\n",
        "    checkpoint_path = \"rtiotcGAN_epoch_{epoch:04d}.h5\"\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=False,\n",
        "        save_freq='epoch',  # 'period' is deprecated\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    evaluation_callback = EvaluationCallback(generator, discriminator, latent_dim, n_classes,\n",
        "                                             validation_data=(images[-10:], labels_encoded[-10:]), interval=100)\n",
        "\n",
        "    # Training loop\n",
        "    cGAN.fit([np.random.normal(0, 1, (len(images), latent_dim)), labels_encoded],\n",
        "             np.ones((len(images), 1)),\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size,\n",
        "             callbacks=[checkpoint_callback, evaluation_callback])\n",
        "\n",
        "    return generator, discriminator, cGAN\n",
        "\n",
        "\n",
        "# Usage (5000 epochs is not a lot for GAN training and may require substantial compute resources)\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "generator, discriminator, cGAN = train_cgan(images, labels_encoded, epochs=100, batch_size=32)\n",
        "\n",
        "# Print Model Summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "cGAN.summary()\n"
      ],
      "metadata": {
        "id": "zEARTLMAGf87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generates images.\n",
        "\n",
        "def define_generator(latent_dim=528, n_classes=12):\n",
        "    noise_input = Input(shape=(latent_dim,))\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, latent_dim)(label_input)\n",
        "    label_embedding = Reshape((latent_dim,))(label_embedding)\n",
        "\n",
        "    # Combined noise and label embeddings\n",
        "    combined_input = Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # Dense layer to map combined input to initial image size\n",
        "    dense_output = Dense(8 * 8 * 128, activation='relu')(combined_input)\n",
        "    reshaped_output = Reshape((8, 8, 128))(dense_output)\n",
        "\n",
        "    # Upsample using transposed convolutional layers\n",
        "    upsample1 = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu')(reshaped_output)\n",
        "    upsample2 = Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='sigmoid')(upsample1)\n",
        "\n",
        "    # Resize the image to the desired output size (e.g., 128x128)\n",
        "    resized_output = Lambda(lambda x: tf.image.resize(x, (128, 128)))(upsample2)\n",
        "\n",
        "    # Define generator model\n",
        "    generator_model = Model([noise_input, label_input], resized_output)\n",
        "    return generator_model\n",
        "\n",
        "def define_discriminator(input_shape=(128, 128, 3), n_classes=12):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    label_input = Input(shape=(1,))\n",
        "\n",
        "    # Embedding layer for conditional input\n",
        "    label_embedding = Embedding(n_classes, np.prod(input_shape))(label_input)\n",
        "    label_embedding = Reshape(input_shape)(label_embedding)\n",
        "\n",
        "    # Resize the image to match the label embedding shape (if needed)\n",
        "    resized_image = Lambda(lambda x: tf.image.resize(x, input_shape[:2]))(image_input)\n",
        "\n",
        "    # Concatenate resized image and label embedding\n",
        "    combined_input = Concatenate()([resized_image, label_embedding])\n",
        "\n",
        "    # Convolutional layers\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same', activation='relu')(combined_input)\n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten and final classification layer\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define discriminator model\n",
        "    discriminator_model = Model([image_input, label_input], output)\n",
        "    return discriminator_model\n",
        "\n",
        "def extract_label_from_filename(filename):\n",
        "    # Example: \"ARP_poisoning_12707.png\"\n",
        "    label = filename.split('_')[0]  # Extract label before the first underscore\n",
        "    return label\n",
        "\n",
        "def encode_labels(labels):\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    encoded_labels = [label_to_id[label] for label in labels]\n",
        "    return np.array(encoded_labels)\n",
        "\n",
        "def load_custom_dataset(dataset_path, target_size=(128, 128), batch_size=32):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith('.png'):\n",
        "            image_path = os.path.join(dataset_path, filename)\n",
        "            label = extract_label_from_filename(filename)\n",
        "\n",
        "            # Load and preprocess image\n",
        "            img = load_img(image_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert labels to encoded integers\n",
        "    labels_encoded = encode_labels(labels)\n",
        "\n",
        "    # Return images and encoded labels\n",
        "    return np.array(images), labels_encoded\n",
        "\n",
        "def generate_images(model, test_input, n_classes=10):\n",
        "    # Initialize output variable to store generated images\n",
        "    output = None\n",
        "\n",
        "    # Generate images for each class label\n",
        "    for label in range(n_classes):\n",
        "        labels = np.ones((test_input.shape[0],)) * label\n",
        "        predictions = model.predict([test_input, labels])\n",
        "\n",
        "        if output is None:\n",
        "            output = predictions\n",
        "        else:\n",
        "            output = np.concatenate((output, predictions))\n",
        "\n",
        "    # Configure plotting parameters\n",
        "    nrow = n_classes\n",
        "    ncol = test_input.shape[0]\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    gs = fig.add_gridspec(nrow, ncol, wspace=0, hspace=0)\n",
        "\n",
        "    # Plot generated images\n",
        "    k = 0\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            ax = fig.add_subplot(gs[i, j])\n",
        "            ax.imshow(output[k])\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            ax.set_aspect('auto')\n",
        "            k += 1\n",
        "\n",
        "    # Show the generated images\n",
        "    plt.show()\n",
        "\n",
        "# Define and compile cGAN model\n",
        "latent_dim = 528\n",
        "n_classes = 12\n",
        "epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "generator = define_generator(latent_dim, n_classes)\n",
        "discriminator = define_discriminator()\n",
        "\n",
        "# Compile discriminator\n",
        "discriminator.compile(optimizer=Adam(learning_rate=0.0001, beta_1=0.5),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Combine generator and discriminator into cGAN\n",
        "noise_input = Input(shape=(latent_dim,))\n",
        "label_input = Input(shape=(1,))\n",
        "\n",
        "generated_image = generator([noise_input, label_input])\n",
        "validity = discriminator([generated_image, label_input])\n",
        "\n",
        "cGAN = Model([noise_input, label_input], validity)\n",
        "cGAN.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.5),\n",
        "             loss='binary_crossentropy')\n",
        "\n",
        "# Training loop\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "images, labels_encoded = load_custom_dataset(dataset_path, target_size=(128, 128))\n",
        "\n",
        "# Lists to store losses for plotting\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "# Train cGAN\n",
        "for epoch in range(epochs):\n",
        "    idx = np.random.randint(0, images.shape[0], batch_size)\n",
        "    real_images, labels = images[idx], labels_encoded[idx]\n",
        "\n",
        "    # Generate random noise for the generator input\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Generate fake images using the generator\n",
        "    generated_images = generator.predict([noise, labels])\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch([real_images, labels], np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch([generated_images, labels], np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator (via the cGAN model)\n",
        "    g_loss = cGAN.train_on_batch([noise, labels], np.ones((batch_size, 1)))\n",
        "\n",
        "    # Store losses for plotting\n",
        "    d_losses.append(d_loss[0])\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "    # Print training progress\n",
        "    print(f\"Epoch: {epoch + 1}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), d_losses, label=\"Discriminator Loss\")\n",
        "plt.plot(range(1, epochs + 1), g_losses, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Generate and display sample images\n",
        "test_noise = np.random.normal(size=(10, latent_dim))\n",
        "generate_images(generator, test_noise)\n"
      ],
      "metadata": {
        "id": "FQ0pDQ5YI8Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS MATERIALS\n",
        "\n",
        "# # THE STRUGGLE"
      ],
      "metadata": {
        "id": "XJyB2w0mho0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extras Journey to cgan perfection"
      ],
      "metadata": {
        "id": "-z7-mDBYX_f9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKnvBDnIb2fC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyts.image import GramianAngularField\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the number of samples to select for each label (up to 400)\n",
        "samples_per_label = 400\n",
        "\n",
        "# Total number of images to save\n",
        "total_images_to_save = 4800\n",
        "\n",
        "# Get unique labels (attack types) from 'y'\n",
        "unique_labels = np.unique(y)\n",
        "\n",
        "# Dictionary to store indices of samples for each label\n",
        "label_indices = defaultdict(list)\n",
        "\n",
        "# Populate label_indices with indices of each label in 'y'\n",
        "for idx, label in enumerate(y):\n",
        "    label_indices[label].append(idx)\n",
        "\n",
        "# Initialize Gramian Angular Field transformer\n",
        "gasf = GramianAngularField(image_size=X.shape[1], method='summation')\n",
        "\n",
        "# Define the directory to save images\n",
        "save_dir = os.path.join(\"/content/drive/MyDrive/gasf_images\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Track the number of images saved for each label\n",
        "label_images_saved = defaultdict(int)\n",
        "\n",
        "# Iterate over each label\n",
        "for label in unique_labels:\n",
        "    # Get available indices for the current label\n",
        "    indices = label_indices[label]\n",
        "\n",
        "    # Determine the number of samples to use for this label\n",
        "    num_samples = min(samples_per_label, len(indices), total_images_to_save - sum(label_images_saved.values()))\n",
        "\n",
        "    if num_samples > 0:\n",
        "        # Randomly select up to `num_samples` indices for this label\n",
        "        selected_indices = np.random.choice(indices, size=num_samples, replace=False)\n",
        "\n",
        "        for idx in selected_indices:\n",
        "            # Extract the features (all columns) for the current row\n",
        "            row_data = X[idx]\n",
        "\n",
        "            # Apply Gramian Angular Field transformation to the entire row\n",
        "            gasf_image = gasf.fit_transform(row_data.reshape(1, -1))\n",
        "            gasf_image = np.squeeze(gasf_image)\n",
        "\n",
        "            # Scale the GAF image to [0, 1]\n",
        "            scaled_image = (gasf_image - gasf_image.min()) / (gasf_image.max() - gasf_image.min())\n",
        "\n",
        "            # Create an RGB image using the 'inferno' colormap\n",
        "            colored_image = plt.get_cmap('inferno')(scaled_image)\n",
        "\n",
        "            # Construct image filename and path\n",
        "            image_filename = f'image_label_{label}_idx_{idx}.png'\n",
        "            image_path = os.path.join(save_dir, image_filename)\n",
        "\n",
        "            # Save the RGB image\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.imshow(colored_image, aspect='auto')  # Display the RGB image\n",
        "            plt.axis('off')  # Turn off axis\n",
        "            plt.savefig(image_path, bbox_inches='tight', pad_inches=0)  # Save image without extra whitespace\n",
        "            plt.close()  # Close the plot to release memory\n",
        "\n",
        "            # Increment the count of images saved for this label\n",
        "            label_images_saved[label] += 1\n",
        "\n",
        "            print(f'Saved: {image_filename}')\n",
        "\n",
        "print(f'Images saved to: {save_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tie4IAC413m1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pyts.image import MarkovTransitionField\n",
        "\n",
        "# Step 1: Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Define the MTF transformer\n",
        "mtf = MarkovTransitionField(n_bins=8)\n",
        "\n",
        "# Step 3: Preprocess training data with MTF and keep track of labels\n",
        "mtf_images_train = mtf.fit_transform(X_train)\n",
        "\n",
        "# Step 4: Visualize MTF images for a few samples with labels\n",
        "num_samples_to_visualize = 20  # Number of samples per class to visualize\n",
        "classes = np.unique(y_train)  # Unique classes in the training labels\n",
        "\n",
        "# Sample and display images from each class\n",
        "for cls in classes:\n",
        "    cls_indices = np.where(y_train == cls)[0]  # Indices of samples belonging to the current class\n",
        "    sampled_indices = np.random.choice(cls_indices, num_samples_to_visualize, replace=False)\n",
        "    for idx in sampled_indices:\n",
        "        plt.imshow(mtf_images_train[idx], cmap='rainbow', origin='lower', vmin=0., vmax=1.)\n",
        "        plt.title(f\"MTF Image for Sample {idx+1} (Class: {y_train[idx]})\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "# Step 5: Preprocess test data with MTF\n",
        "mtf_images_test = mtf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNyBX2DRy8G6"
      },
      "outputs": [],
      "source": [
        "#Loader Code\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/RT_IOT2022 DATA/gasfimages/'\n",
        "\n",
        "# List all files in the directory\n",
        "file_list = os.listdir(dataset_path)\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for filename in file_list:\n",
        "    if filename.endswith('.png'):  # Assuming all images are PNG format\n",
        "        # Load image\n",
        "        image_path = os.path.join(dataset_path, filename)\n",
        "        image = Image.open(image_path)\n",
        "        # Preprocess image (resize, normalize, etc.)\n",
        "        image = image.resize((224, 224))  # Resize to target size\n",
        "        image = np.array(image) / 255.0   # Normalize pixel values to [0, 1]\n",
        "        # Extract label from filename\n",
        "        label = filename.split('_')[2]    # Extract label from filename (e.g., 'label_10')\n",
        "        label = int(label.split('.')[0])  # Convert label to integer (e.g., 10)\n",
        "        # Append to lists\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}